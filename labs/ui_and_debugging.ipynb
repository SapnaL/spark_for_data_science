{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkUI and Debugging Lab\n",
    "\n",
    "Run the following code. This will do an import needed for later, but also start up the Spark session. Click the link to get to the Spark UI. You will need to replace `http://spark-training-primary.umbctraining.com` with `http://54.156.199.198` in the link that opens up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the following code, then head over to the SparkUI and take a look at what happens while code is running. This code will take about 5 minutes to run, so you should have time to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = spark.read.csv(\"hdfs:///user/bryan/data/name.basics.tsv\",header=True,inferSchema=True,sep='\\t')\n",
    "players = players.filter(players.primaryProfession.rlike(\"act\")).filter((players.deathYear == '\\\\N') & (players.birthYear != '\\\\N')).filter(players.birthYear.cast('int') > 1950)\n",
    "\n",
    "players2 = spark.read.csv(\"hdfs:///user/bryan/data/name.basics.tsv\",header=True,inferSchema=True,sep='\\t')\n",
    "for c in players2.columns:\n",
    "    players2 = players2.withColumnRenamed(c, c + \"_2\")    \n",
    "players2 = players2.filter(players2.primaryProfession_2.rlike(\"act\")).filter((players2.deathYear_2 == '\\\\N') & (players2.birthYear_2 != '\\\\N')).filter(players2.birthYear_2.cast('int') > 1950)\n",
    "\n",
    "print(players2.count())\n",
    "\n",
    "joined = players.crossJoin(players2)\n",
    "\n",
    "print(joined.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section, fix the bug. Even if you can spot the bug now, run the code so you can pratice reading error statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_4(column):\n",
    "    return column.split('-')[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"hdfs:///user/bryan/data/100_percent_real_data.csv\",inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_last_4_udf = spark.udf.register(\"get_last_4\",get_last_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"last_4\",get_last_4_udf(data.phone))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the code below has a bug in it. Run it until you get the bug, and then try to use the error statements to fix the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_address(df):\n",
    "    return df.assign(address = df['adress'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_udf = pandas_udf(same_address, StructType([StructField(\"name\",StringType()),\n",
    "                                StructField(\"address\",StringType()),\n",
    "                                StructField(\"company\",StringType()),\n",
    "                                StructField(\"phone\",StringType()),\n",
    "                                StructField(\"ssn\",StringType())]),\n",
    "                              PandasUDFType.GROUPED_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('company').apply(same_udf).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
